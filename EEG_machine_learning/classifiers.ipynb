{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Events with:\n",
    "* Logistic Regression\n",
    "* Random Forests\n",
    "* SVM\n",
    "* Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use mne XDawn for preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import mne\n",
    "from mne import io, pick_types, read_events, Epochs\n",
    "# from mne.datasets import sample\n",
    "from mne.preprocessing import Xdawn\n",
    "from mne.decoding import Vectorizer\n",
    "from mne.viz import tight_layout\n",
    "\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "\n",
    "#data path for each run of each subject.\n",
    "drive_data_path = 'E:\\eeg_data'\n",
    "\n",
    "#data path on my external hdd for folder containing all tests of each subject in one file\n",
    "drive_all_data_path = 'E:\\eeg_data\\ica_140_500_0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import all runs data from 10 subjects\n",
    "\n",
    "#initalize a np array of numbers from 1 to 5. This is the number of subjects\n",
    "np.random.seed(41)\n",
    "numb_subj = np.random.randint(1,25,5)\n",
    "numb_subj\n",
    "print(\"subject number used for classification:\",numb_subj)\n",
    "#empty array that will have all file names\n",
    "files = []\n",
    "\n",
    "for i in range(len(numb_subj)):\n",
    "    files.append(str('subject' + str(numb_subj[i]) + '_all_runs-epo.fif'))\n",
    "\n",
    "#convert to np array\n",
    "files = np.array(files)\n",
    "\n",
    "all_data = []\n",
    "for i in range(len(files)):\n",
    "     all_data.append(mne.read_epochs(op.join(drive_all_data_path, files[i]),\n",
    "                          preload=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract event_id 4 and 5 from all data\n",
    "\n",
    "all_epochs = []\n",
    "for i in range(len(all_data)):\n",
    "    all_epochs.append(all_data[i][(all_data[i].events[:,-1] == 4) | (all_data[i].events[:,-1] == 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate epochs list\n",
    "epochs = mne.concatenate_epochs(all_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epochs.pick_channels(epochs.ch_names[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(Xdawn(n_components = 3),\n",
    "                    Vectorizer(),\n",
    "                    MinMaxScaler(),\n",
    "                    LogisticRegression(penalty='l1'))\n",
    "#cross validator\n",
    "cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "#Do cross-validation\n",
    "labels = epochs.events[:,-1]\n",
    "preds = np.empty(len(labels))\n",
    "\n",
    "for train, test in cv.split(epochs, labels):\n",
    "    clf.fit(epochs[train], labels[train])\n",
    "    preds[test] = clf.predict(epochs[test])\n",
    "    \n",
    "target_names = ['new', 'scramble']\n",
    "report = classification_report(labels, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "cm_normalized = cm.astype(float) / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.imshow(cm_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Normalized Confusion matrix', fontsize = 35)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(target_names))\n",
    "plt.xticks(tick_marks, target_names, rotation=45, fontsize = 20)\n",
    "plt.yticks(tick_marks, target_names, fontsize = 20)\n",
    "tight_layout()\n",
    "plt.ylabel('True label', fontsize = 35)\n",
    "plt.xlabel('Predicted label',fontsize = 35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(epochs[test],labels[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More tests by sampling more files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "subjects = []\n",
    "#generate 10 random samples of 5 files \n",
    "for i in range(10):\n",
    "    subjects.append(np.random.choice(25,5))\n",
    "subjects = np.array(subjects)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file\n",
    "def read_file(path, file_name):\n",
    "    return mne.read_epochs(op.join(path, file_name),\n",
    "                          preload=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_new_scrambled(epochs):\n",
    "    \"\"\"Filter events in epoch data\n",
    "    epochs: mne.epochs.EpochsFIF object \n",
    "    \"\"\"\n",
    "    \n",
    "    return epochs[(epochs.events[:,-1] == 4) | (epochs.events[:,-1] == 5)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = read_file(drive_all_data_path,'subject1_all_runs-epo.fif');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = filter_new_scrambled(epochs)\n",
    "#Having 61 channels does not work when classifiying for some reason. It seems like any number less than 61 works. \n",
    "epochs = epochs.pick_channels(epochs.ch_names[:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to classify one *all runs* file\n",
    "* using 60 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(Xdawn(n_components = 3),\n",
    "                    Vectorizer(),\n",
    "                    MinMaxScaler(),\n",
    "                    LogisticRegression(penalty='l2'))\n",
    "#cross validator\n",
    "cv = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "\n",
    "#Do cross-validation\n",
    "labels = epochs.events[:,-1]\n",
    "preds = np.empty(len(labels))\n",
    "\n",
    "for train, test in cv.split(epochs, labels):\n",
    "    clf.fit(epochs[train], labels[train])\n",
    "    preds[test] = clf.predict(epochs[test])\n",
    "    \n",
    "target_names = ['new', 'scramble']\n",
    "report = classification_report(labels, preds, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(epochs[test],labels[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to classify the same file but with one channel\n",
    "* How will classification perform if only one channel is used?\n",
    "* I will choose one random channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "chan_numb = np.random.choice(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_numb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract one channel\n",
    "epochs.pick_channels(epochs.ch_names[chan_numb:chan_numb+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations of the plots\n",
    "* Looking at 200ms sections of each plot and trying to see if there are clear differences between the events.\n",
    "* at 600 - 800ms, all but the bottom left have a high differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 2, ncols = 2, figsize = (25,15), )\n",
    "\n",
    "fig.suptitle(\"Plots of Pairs of New and Scrambled Events of Channel {}\".format(epochs.ch_names[-1]), fontsize=20);\n",
    "\n",
    "axs[0,0].plot(epochs._data[0][0], label = epochs.events[0][-1])\n",
    "axs[0,0].plot(epochs._data[1][0], label = epochs.events[1][-1])\n",
    "\n",
    "axs[0,1].plot(epochs._data[3][0], label = epochs.events[3][-1])\n",
    "axs[0,1].plot(epochs._data[4][0], label = epochs.events[4][-1])              \n",
    "\n",
    "axs[1,0].plot(epochs._data[5][0], label = epochs.events[5][-1])\n",
    "axs[1,0].plot(epochs._data[6][0], label = epochs.events[6][-1])              \n",
    "\n",
    "axs[1,1].plot(epochs._data[8][0], label = epochs.events[8][-1])\n",
    "axs[1,1].plot(epochs._data[9][0], label = epochs.events[9][-1])              \n",
    "\n",
    "axs[0,0].legend()\n",
    "axs[0,1].legend()\n",
    "axs[1,0].legend()\n",
    "axs[1,1].legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the epoched data of one channel into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize np array of shape epoch length and time of 1400 ms.\n",
    "np_epochs = np.empty(shape = (len(epochs),1401))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[0]._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign values of each epoch to np array\n",
    "for i in range(len(epochs)):\n",
    "    np_epochs[i] = epochs[i]._data\n",
    "    \n",
    "# pd.DataFrame(epochs._data.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas df\n",
    "epoch_df = pd.DataFrame(np_epochs)\n",
    "# add event column\n",
    "epoch_df['event'] = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a column for each millisecond\n",
    "epoch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X values are the 1400 columns\n",
    "X = epoch_df.iloc[:,:-1].values\n",
    "# y values are events\n",
    "y = epoch_df.iloc[:,-1:].values\n",
    "#split data into training/ test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .30, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize classifier\n",
    "clf = LogisticRegression(penalty='l1')\n",
    "#fit data\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score..is not good\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with what the features and samples are.\n",
    "* This is not a good idea, but nonetheless offers insight on what not to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
