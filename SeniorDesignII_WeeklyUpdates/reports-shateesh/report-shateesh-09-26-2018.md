# Smart and Stretch Goals

**Name:** Shateesh Bhugwansing
**Date:** 09/26/18

## Stretch Goals (1-3)

1. Conduct controlled experiments with ML algorithms on epoched data to find the best classification pipeline to use. 


## S.M.A.R.T. Goals (next week)

### S.M.A.R.T. Goal 1.

#### S. Specific: 
Continue the classification experiment from last week using a logistic regression classifier. Combine data from one test subject into general flanker vs. target stimuli and try to classify. 

#### M. Measurable: 
I can measure the success of the logistic regression classifier by using the built-in sklearn .score function and a confusion matrix. 

#### A. Achievable: 
Yes, the skills required have been taught in semester 1 of senior design. 

#### R. Relevant :
This goal will help our team decide on what classification algorithm to use in the future. 


#### T. Time-bound: 
It is due next week: 10/03/18

### S.M.A.R.T. Goal 2.

#### S. Specific: 
I will run the same experiement in SMART goal #1, but with a SVM classifier. 

#### M. Measurable: 
Like SMART goal #1, I will measure the goal using built-in sklearn accuracy functions. 

#### A. Achievable: 
Yes, SVM documentation is plentiful on the sklearn website. 

#### R. Relevant :
This will help our team decide on which classifier to use in the future. 


#### T. Time-bound: 
Next week, 10/03/18

### S.M.A.R.T. Goal 3.

#### S. Specific: 
I aim to reshape the data by filtering the PCA-reduced data according to their brain wave type, and run both classifiers from goals #1 and #2 and compare results. 

#### M. Measurable: 
I will measure this goal using the accuracy functions that are built-in in sklearn. 

#### A. Achievable: 
Yes, since all of the documentation is on the sklearn website. 

#### R. Relevant :
This will help our team decide on which classifier to use in the future. 


#### T. Time-bound: 
I aim for next week, 10/03/18. BUT this depends on how far I get with goals #1 and #2


## S.M.A.R.T. Goals (last week)

1. Perform dimensionality reduction: PCA worked. I was able to reduce the data from 129 channels to 9. 
2. I did not do bandpass filtering last week because Tarekul did it. 
3. I used the KNN classifer, but got an average score of ~ 54%. 
