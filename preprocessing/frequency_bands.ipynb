{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../preprocessing/Artifact_Removal/preprocecssing_helpers.ipynb\n",
    "%run ../preprocessing/StimCodes.ipynb\n",
    "%run ../preprocessing/Artifact_Removal/Batch_ArtifactFilter_Epoch.ipynb\n",
    "%run ../Classification/ConcatEpochTrails.ipynb\n",
    "%run ../preprocessing/Artifact_Removal/Extract_Describer_Events.ipynb\n",
    "%run ../Classification/ConcatEpochTrails.ipynb\n",
    "%run ../PCA/Emmanuil-PCA.ipynb\n",
    "\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import mne\n",
    "from mne.baseline import rescale\n",
    "from mne.stats import _bootstrap_ci\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mne.viz import tight_layout\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "matplotlib.rcParams.update({'font.size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:\\stim_code_epochs'\n",
    "epoch_files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n = np.random.randint(0,len(epoch_files))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = mne.read_epochs(os.path.join(path, epoch_files[5]), preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.events[900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "493165 * 1/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_freqs = [\n",
    "    ('Theta', 4, 7),\n",
    "    ('Alpha', 8, 12),\n",
    "    ('Beta', 13, 25),\n",
    "    ('Gamma', 30, 45)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions of n_times to a point for each band.\n",
    "montage = mne.channels.read_montage(kind = 'ANT_DukeWaveGuard_128_electrode_montages_updated_V4')\n",
    "epoch.set_montage(montage);\n",
    "if 'Nasium' in epoch.ch_names:\n",
    "    epoch.drop_channels(ch_names=['Nasium', 'LL4', 'L12', 'VEOG']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.plot_sensors(show_names=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events = convert_epoch_events_to_stim_combinations(epoch)\n",
    "print(new_events.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.events = new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create arrays based on audio and visual\n",
    "audio_epochs = []\n",
    "visual_epochs = []\n",
    "\n",
    "for i in range(len(epoch)): \n",
    "#     print(i)\n",
    "    if epoch[i].events[:,-1][0] <700:\n",
    "#         print(\"audio\")\n",
    "        audio_epochs.append(epoch[i].get_data())\n",
    "    else:\n",
    "#         print(\"visual\")\n",
    "        visual_epochs.append(epoch[i].get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(Vectorizer(), StandardScaler())\n",
    "scaled_audio_data = pipeline.fit_transform(audio_epochs)\n",
    "scaled_visual_data = pipeline.fit_transform(visual_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = make_pipeline(Vectorizer())\n",
    "vectorized_audio_data = vectorizer.fit_transform(audio_epochs)\n",
    "vectorized_visual_data = vectorizer.fit_transform(visual_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_audio_data = preprocessing.scale(vectorized_audio_data)\n",
    "scaled_visual_data = preprocessing.scale(vectorized_visual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_variance = []\n",
    "for data in scaled_audio_data:\n",
    "#     print(data)\n",
    "    audio_variance.append(np.var(data))\n",
    "visual_variance = []\n",
    "for data in scaled_visual_data:\n",
    "    visual_variance.append(np.var(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variance = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(audio_variance).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_visual_labels(epoch_object):\n",
    "    events = epoch_object.events[:,-1]\n",
    "    labels = []\n",
    "    for event in events:\n",
    "        if event < 700:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return np.array(labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracting frequency bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_band(band, epoch_object):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        returns an epoch object with a filtered frequency band\n",
    "    \n",
    "    Variables: \n",
    "        band : \n",
    "            'Theta':(4,7),\n",
    "            'Alpha':(8,12),\n",
    "            'Beta':(13,25),\n",
    "            'Gamma':(30,45)\n",
    "        epoch_object: mne.epoch\n",
    "    -----\n",
    "    returns: epoch object\n",
    "    \n",
    "    \"\"\"\n",
    "    iter_freqs = {\n",
    "        'Theta':(4,7),\n",
    "        'Alpha':(8,12),\n",
    "        'Beta':(13,25),\n",
    "        'Gamma':(30,45)\n",
    "    }\n",
    "    copy_epoch = epoch_object.copy()\n",
    "    copy_epoch.filter(l_freq= iter_freqs[band][0],\n",
    "                    h_freq = iter_freqs[band][1])\n",
    "    \n",
    "    return copy_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_epoch = get_frequency_band('Gamma', epoch)\n",
    "theta_epoch = get_frequency_band('Theta', epoch)\n",
    "beta_epoch = get_frequency_band('Beta', epoch)\n",
    "alpha_epoch = get_frequency_band('Alpha',epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_of_epochs(epoch_object):\n",
    "    pipeline = make_pipeline(Vectorizer(), StandardScaler())\n",
    "    \n",
    "    scaled_epoch_data = pipeline.fit_transform(epoch_object.get_data())\n",
    "    \n",
    "    variance_epochs = []\n",
    "    for data in scaled_epoch_data:\n",
    "        variance_epochs.append(np.var(data))\n",
    "    \n",
    "    return np.array(variance_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_epochs(epoch_object):\n",
    "    pipeline = make_pipeline(Vectorizer(), StandardScaler())\n",
    "    \n",
    "    scaled_epoch_data = pipeline.fit_transform(epoch_object.get_data())\n",
    "    \n",
    "    mean_epochs = []\n",
    "    for data in scaled_epoch_data:\n",
    "        mean_epochs.append(np.mean(data))\n",
    "    \n",
    "    return np.array(mean_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_gamma_epochs = variance_of_epochs(gamma_epoch)\n",
    "variance_theta_epochs = variance_of_epochs(theta_epoch)\n",
    "variance_beta_epochs = variance_of_epochs(beta_epoch)\n",
    "variance_alpha_epochs = variance_of_epochs(alpha_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_gamma_epochs = variance_gamma_epochs[:-1]\n",
    "variance_theta_epochs = variance_theta_epochs[:-1]\n",
    "variance_beta_epochs = variance_beta_epochs[:-1]\n",
    "variance_alpha_epochs = variance_alpha_epochs[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_gamma_epochs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_audio_visual_labels(gamma_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_epoch.events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(variance_alpha_epochs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(variance_gamma_epochs[labels == 0], \n",
    "            variance_theta_epochs[labels == 0], c = 'red', marker = 'x');\n",
    "plt.scatter(variance_gamma_epochs[labels == 1], \n",
    "            variance_theta_epochs[labels == 1], c = 'blue', marker = 'o');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(variance_beta_epochs[labels == 0], \n",
    "            variance_alpha_epochs[labels == 0], c = 'red', marker = 'x');\n",
    "plt.scatter(variance_beta_epochs[labels == 1], \n",
    "            variance_alpha_epochs[labels == 1], c = 'blue', marker = 'o');\n",
    "plt.scatter(variance_gamma_epochs[labels == 0], \n",
    "            variance_theta_epochs[labels == 0], c = 'k', marker = 's');\n",
    "plt.scatter(variance_gamma_epochs[labels == 1], \n",
    "            variance_theta_epochs[labels == 1], c = 'green', marker = '_');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.plot_psd_topomap(ch_type='eeg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.plot_psd_topomap(ch_type='eeg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.plot_image();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_channels = ['RC7','RD7','RE4','R12','R13','RR12','RR13',\n",
    "                   'LC7','LD7',';LE4','L12','L13','LL12','LL13']\n",
    "audio_channels = [x for x in epoch.ch_names if 'LD' in x or 'LC' in x or 'LA' in x\n",
    "                  or 'RD' in x or 'RC' in x or 'RA' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_channels = []\n",
    "for ch in epoch.ch_names:\n",
    "    if ch not in visual_channels and ch not in audio_channels:\n",
    "        bad_channels.append(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.drop_channels(ch_names= bad_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_epoch = get_frequency_band('Gamma', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(Vectorizer(), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_stats = get_mean_band(gamma_epoch[:-1])\n",
    "mean_stats = mean_stats.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_visual_events = []\n",
    "for event in new_events:\n",
    "    if event[-1] < 700:\n",
    "        event[-1] = 100\n",
    "    else:\n",
    "        event[-1] = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.events = new_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification pipeline\n",
    "start = time.time()\n",
    "clf = make_pipeline(Vectorizer(),\n",
    "                    StandardScaler(),\n",
    "                    LogisticRegression(penalty='l1'))\n",
    "end = time.time()\n",
    "print(\"clf elapsed time: {0}\".format(end - start))\n",
    "\n",
    "start_master = time.time()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "labels = epoch.events[:,-1]\n",
    "preds = np.empty(len(mean_stats))\n",
    "for train, test in cv.split(mean_stats, labels):\n",
    "    start = time.time()\n",
    "    clf.fit(mean_stats[train], labels[train])\n",
    "    preds[test] = clf.predict(mean_stats[test])\n",
    "    end = time.time()\n",
    "    print(\"kfold elapsed time: {0}\".format(end - start))\n",
    "end = time.time()\n",
    "print(\"classification elapsed time: {0}\".format(end - start_master))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "target_names = ['Audio', 'Visual']\n",
    "report = classification_report(labels, preds, target_names=target_names)\n",
    "print(report)\n",
    "end = time.time()\n",
    "print(\"reporting elapsed time: {0}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = mne.read_epochs(os.path.join(path, epoch_files[6]), preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_events = convert_epoch_events_to_stim_combinations(test_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_gamma = get_frequency_band('Gamma', test_epoch)\n",
    "test_epoch_gamma = test_epoch_gamma[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch_gamma.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in test_events:\n",
    "    if event[-1] < 700:\n",
    "        event[-1] = 100\n",
    "    else:\n",
    "        event[-1] = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_scale = make_pipeline(Vectorizer(),\n",
    "                    StandardScaler())\n",
    "scaled_test_epoch_gamma_data = vect_scale.fit_transform(test_epoch_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(scaled_test_epoch_gamma_data, test_events[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3 = confusion_matrix(labels, preds)\n",
    "cm3_normalized = cm3.astype(float) / cm3.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.imshow(cm3_normalized, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Audio vs. Visual with Audio and Visual Related Channels', size = 20)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(target_names))\n",
    "plt.xticks(tick_marks, target_names, rotation=45, size = 20)\n",
    "plt.yticks(tick_marks, target_names,size = 20)\n",
    "tight_layout()\n",
    "plt.ylabel('True label',size = 20)\n",
    "plt.xlabel('Predicted label',size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification pipeline\n",
    "start = time.time()\n",
    "clf = make_pipeline(Vectorizer(),\n",
    "                    StandardScaler(),\n",
    "                    LogisticRegression(penalty='l1'))\n",
    "end = time.time()\n",
    "print(\"clf elapsed time: {0}\".format(end - start))\n",
    "\n",
    "start_master = time.time()\n",
    "cv = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n",
    "\n",
    "labels = epoch.events[:,-1]\n",
    "preds = np.empty(len(mean_stats))\n",
    "for train, test in cv.split(mean_stats, labels):\n",
    "    start = time.time()\n",
    "    clf.fit(mean_stats[train], labels[train])\n",
    "    preds[test] = clf.predict(mean_stats[test])\n",
    "    end = time.time()\n",
    "    print(\"kfold elapsed time: {0}\".format(end - start))\n",
    "end = time.time()\n",
    "print(\"classification elapsed time: {0}\".format(end - start_master))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "target_names = ['Audio', 'Visual']\n",
    "report = classification_report(labels, preds, target_names=target_names)\n",
    "print(report)\n",
    "end = time.time()\n",
    "print(\"reporting elapsed time: {0}\".format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
