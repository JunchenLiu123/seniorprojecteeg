{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Wavelet Transformation for Feature Selection \n",
    "Shateesh Bhugwansing\n",
    "\n",
    "In an attempt to increase our classification scores, I'm attempting to use Discrete Wavelet Transformation to decompose the EEG data. The resulting sub bands were ranked using their energy levels, and then fit into classifiers.\n",
    "\n",
    "The following papers were used as a reference for this procedure:   \n",
    "1) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5702353/  \n",
    "2) https://link.springer.com/article/10.1007%2Fs13246-015-0333-x#Sec3  \n",
    "3) https://ieeexplore.ieee.org/document/6997315/\n",
    "\n",
    "### PLEASE NOTE: \n",
    "* the python package 'pywt' (PyWavelet, http://pywavelets.readthedocs.io/en/latest/regression/dwt-idwt.html) and 'mne' must be installed in your environment in order to run this notebook. \n",
    "* the data sources are local to my machine/external hard drive. We haven't set up a cloud storage system yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# the  classifiers used by our group\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "\n",
    "#import mne to read EEG data \n",
    "import mne \n",
    "from mne.decoding import Vectorizer, CSP\n",
    "\n",
    "#import pywavelet for DWT \n",
    "import pywt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run1-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61) active\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    1400.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "280 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "280 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Reading /Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run2-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61) active\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    1400.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "285 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "285 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Reading /Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run3-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61) active\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    1400.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "287 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "287 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Reading /Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run4-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61) active\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    1400.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "293 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "293 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Reading /Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/subject1_all_runs-epo.fif ...\n",
      "Isotrak not found\n",
      "    Read a total of 1 projection items:\n",
      "        Average EEG reference (1 x 61) active\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    1400.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "1121 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1121 matching events found\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n"
     ]
    }
   ],
   "source": [
    "#read in the data for the first 4 runs of subject 1, and all runs for subject1\n",
    "path1 = \"/Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run1-epo.fif\"\n",
    "path2 = \"/Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run2-epo.fif\"\n",
    "path3 = \"/Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run3-epo.fif\"\n",
    "path4 = \"/Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/Ex10_Suj1_Run4-epo.fif\"\n",
    "path_all = \"/Users/shateeshbhugwansing/Desktop/seniorprojecteeg/clean_data_sample/subject1_all_runs-epo.fif\"\n",
    "\n",
    "run1 = mne.read_epochs(path1, preload=True)\n",
    "run2 = mne.read_epochs(path2, preload=True)\n",
    "run3 = mne.read_epochs(path3, preload=True)\n",
    "run4 = mne.read_epochs(path4, preload=True)\n",
    "runs_all = mne.read_epochs(path_all, preload=True)\n",
    "\n",
    "# extract only the event_id's 4-5 for new vs. scrambled \n",
    "run1 = run1[(run1.events[:,-1] == 4) | (run1.events[:,-1] == 5)]\n",
    "run2 = run2[(run2.events[:,-1] == 4) | (run2.events[:,-1] == 5)]\n",
    "run3 = run3[(run3.events[:,-1] == 4) | (run3.events[:,-1] == 5)]\n",
    "run4 = run4[(run4.events[:,-1] == 4) | (run4.events[:,-1] == 5)]\n",
    "runs_all = runs_all[(runs_all.events[:,-1] == 4) | (runs_all.events[:,-1] == 5)]\n",
    "\n",
    "#store the actual data \n",
    "data1 = run1.get_data()\n",
    "data2 = run2.get_data()\n",
    "data3 = run3.get_data()\n",
    "data4 = run4.get_data()\n",
    "data_all = runs_all.get_data()\n",
    "\n",
    "#store labels (used as targets in the classifier algorithms)\n",
    "labels1 = run1.events[:,-1]\n",
    "labels2 = run2.events[:,-1]\n",
    "labels3 = run3.events[:,-1]\n",
    "labels4 = run4.events[:,-1]\n",
    "labels_all = runs_all.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  (557, 61, 1401)\n",
      "Approximation Coefficients 1:  (557, 61, 704)\n",
      "Detail Coefficients 1:  (557, 61, 704)\n",
      "\n",
      "\n",
      "Approximation Coefficients 2:  (557, 61, 355)\n",
      "Detail Coefficients 2:  (557, 61, 355)\n",
      "\n",
      "\n",
      "Approximation Coefficients 3:  (557, 61, 181)\n",
      "Detail Coefficients 3:  (557, 61, 181)\n",
      "\n",
      "\n",
      "Approximation Coefficients 4:  (557, 61, 94)\n",
      "Detail Coefficients 4:  (557, 61, 94)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# perform DWT on subj1_run1 \n",
    "# the papers used the wavelet Daubechies wavelet (db4), since it is the best for EEG data [3]. so I'll start with that.\n",
    "\n",
    "cA1, cD1 = pywt.dwt(data_all, 'db4')\n",
    "print 'data: ', data_all.shape\n",
    "print 'Approximation Coefficients 1: ', cA1.shape\n",
    "print 'Detail Coefficients 1: ', cD1.shape\n",
    "print \"\\n\"\n",
    "\n",
    "# They also continue to use DWT on cA of each level of decomposition, shown below. \n",
    "# Eventually there will be cA1-4 and cD1-4\n",
    "\n",
    "cA2, cD2 = pywt.dwt(cA1, 'db4')\n",
    "print 'Approximation Coefficients 2: ', cA2.shape\n",
    "print 'Detail Coefficients 2: ', cD2.shape\n",
    "print \"\\n\"\n",
    "\n",
    "cA3, cD3 = pywt.dwt(cA2, 'db4')\n",
    "print 'Approximation Coefficients 3: ', cA3.shape\n",
    "print 'Detail Coefficients 3: ', cD3.shape\n",
    "print \"\\n\"\n",
    "\n",
    "cA4, cD4 = pywt.dwt(cA3, 'db4')\n",
    "print 'Approximation Coefficients 4: ', cA4.shape\n",
    "print 'Detail Coefficients 4: ', cD4.shape\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help! I'm stuck\n",
    "\n",
    "So after reading the papers (links listed at the top of the notebook), I'm still not sure what to do with the coefficients. Do I classify just the coefficients? Do I apply them to the original signal, somehow? I need help from soneone who understands how DWT and Relative wavelet energy is used to actually select features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5178571428571429"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make pipeline \n",
    "clf = make_pipeline(Vectorizer(),\n",
    "                   StandardScaler(),\n",
    "                   GaussianNB())\n",
    "\n",
    "# cross-validation\n",
    "# cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(cD1,labels_all, test_size = 0.3, random_state = 42)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# preds1 = np.empty(len(labels1))\n",
    "\n",
    "#fit the classifier + make predictions for StratifiedKfold \n",
    "'''\n",
    "for train, test in cv.split(cD1, labels1):\n",
    "    clf.fit(cD1[train], labels1[train])\n",
    "    preds1[test] = clf.predict(cD1[test])\n",
    "'''\n",
    "clf.score(X_test,y_test)\n",
    "# print(\"Attempt 1, cD1: accuracy (%): \", metrics.accuracy_score(labels1, preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-46957485f80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# maybe I'm supposed to concatenate the coefficients together?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_cD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcD1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcD2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# maybe I'm supposed to concatenate the coefficients together? \n",
    "all_cD = np.concatenate(cD1[0][0].flatten(), cD2[0][0].flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
