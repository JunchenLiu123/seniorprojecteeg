{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the variances across each channels\n",
    "* this notebook will write methods for finding the variance across each channels by deviding the 500 ms into 10 segments with each segments contains the variance for 50 ms.\n",
    "* The goal is to be able to see which channels are the most actives during the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mne.decoding import SPoC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n",
    "                          cross_val_multiscore, LinearModel, get_coef)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE \n",
    "import os\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "import os.path as op\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from scipy.stats import skew, kurtosis\n",
    "import mne \n",
    "\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC  # noqa\n",
    "from sklearn.model_selection import ShuffleSplit  # noqa\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mne.viz import tight_layout\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Batch_ArtifactFilter_Epoch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Reading C:\\USB Drive\\NewEEG-200s\\SA20140206\\20140206_1151.fdt\n",
      "Reading 0 ... 831247  =      0.000 ...  1623.529 secs...\n",
      "5977 events found\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  31 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 155 156 157 158 159 160 161 162 163 164 165]\n",
      "time difference 1.80859375\n",
      "[509034      0      4]\n",
      "[509960      0    130]\n",
      "\n",
      " **1 bad trials dropped**\n",
      "Setting up band-stop filter\n",
      "Filter length of 3379 samples (6.600 sec) selected\n",
      "Setting up band-pass filter from 5 - 1e+02 Hz\n",
      "l_trans_bandwidth chosen to be 2.0 Hz\n",
      "h_trans_bandwidth chosen to be 25.0 Hz\n",
      "Filter length of 845 samples (1.650 sec) selected\n",
      "5977 events found\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  31 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 155 156 157 158 159 160 161 162 163 164 165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Done  52 tasks      | elapsed:   30.5s\n",
      "[Parallel(n_jobs=10)]: Done 129 out of 129 | elapsed:   47.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5584 events found\n",
      "Event IDs: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  31 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 142 143 144 145 155 156 157 158 159 160 161 162 163 164 165]\n",
      "Fitting ICA to data using 128 channels (please be patient, this may take a while)\n",
      "Inferring max_pca_components from picks\n",
      "Using all PCA components: 128\n",
      "Fitting ICA took 132.9s.\n",
      "    Searching for artifacts...\n",
      "Artifact indices found:\n",
      "    \n",
      "Ready.\n",
      "Transforming to ICA space (128 components)\n",
      "Zeroing out 0 ICA components\n",
      "1979 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 1979 events and 500 original time points ...\n",
      "1011 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "epoch=remove_artifacts_and_create_epochs('C:\\\\USB Drive\\\\NewEEG-200s\\\\SA20140206\\\\20140206_1151.set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\USB Drive\\NewEEG-200s\\SA20140206\\20140206_1151.fdt\n",
      "Reading 0 ... 831247  =      0.000 ...  1623.529 secs...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RawEEGLAB  |  20140206_1151.fdt, n_channels x n_times : 129 x 831248 (1623.5 sec), ~818.4 MB, data loaded>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mne.io.read_raw_eeglab('C:\\\\USB Drive\\\\NewEEG-200s\\\\SA20140206\\\\20140206_1151.set',preload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getave(data,i,j):\n",
    "    dataof100=[]\n",
    "    for channel in data[j]:\n",
    "        dataof100.append(channel[i:i+50])\n",
    "\n",
    "    arrayofdata100=np.array(dataof100)\n",
    "    print(arrayofdata100.shape)\n",
    "    sum1=[]\n",
    "    for channel in arrayofdata100:\n",
    "        sum1.append(np.sum(channel))\n",
    "    sum1=np.array(sum1)\n",
    "    mean1=np.mean(sum1)\n",
    "    mean2=np.sum(sum1)/129\n",
    "    var1=sum1-mean1\n",
    "    print(\"variance\",var1)\n",
    "    average1=np.mean(var1)\n",
    "    print(\"average varaince for segment \" ,i ,\"to \", i+49 , \"is: \" ,average1)\n",
    "#     df=pd.DataFrame(var1)\n",
    "#     print(df)\n",
    "#     df.columns=['variance']\n",
    "#     index = np.arange(125)\n",
    "#     df.plot.bar(x=index,y='variance')\n",
    "    \n",
    "    index = np.arange(125)\n",
    "    plt.bar(index, var1)\n",
    "    plt.xlabel('Genre', fontsize=5)\n",
    "    plt.ylabel('No of Movies', fontsize=5)\n",
    "    plt.xticks(index, var1, fontsize=5, rotation=30)\n",
    "    plt.title('average 10 points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,51,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,101,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.drop_channels(['Lm','Rm','VEOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n",
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.drop_channels(['STI 014'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,451,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_KNN(X,y):\n",
    "\n",
    "    clf = make_pipeline(Vectorizer(),\n",
    "                         KNN(n_neighbors=11))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    score = clf.score(X_test, y_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_targets(epochs):\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.events[:,-1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,101)\n",
    "print(X.shape)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,151)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,301)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch)\n",
    "classify_test_KNN(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_event_id(epoch):\n",
    "    epochs=[[]]\n",
    "    j=0\n",
    "    for i in range(len(epoch.events[:,-1])):\n",
    "        if i % 3 ==0:\n",
    "            epochs.append([])\n",
    "            j=j+1\n",
    "            epochs[j].append(epoch.events[i,-1])\n",
    "        else:\n",
    "            epochs[j].append(epoch.events[i,-1])\n",
    "    for events in epochs:\n",
    "        if  4 in events or 1 in events or 13 in events or 16 in events:\n",
    "            for i in range(len(events)):\n",
    "                    events[i]=4\n",
    "        else:\n",
    "            for i in range(len(events)):\n",
    "                    events[i]=5\n",
    "    epochs.pop(0)\n",
    "    flattened_list = [y for x in epochs for y in x]\n",
    "    for i in range(len(epoch.events)):\n",
    "        epoch.events[i]=flattened_list[i]\n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=filter_event_id(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch[0:10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_log(X,y):\n",
    "\n",
    "    clf = make_pipeline(Vectorizer(),\n",
    "                         LogisticRegression())\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    score = clf.score(X_test, y_test)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,101)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,101)\n",
    "classify_test_log(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch,121)\n",
    "classify_test_log(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=get_samples_targets(epoch)\n",
    "X.shape\n",
    "classify_test_log(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getave(data,i,j):\n",
    "    dataof50=[]\n",
    "    for channel in newdata[j]:\n",
    "        dataof50.append(channel[i:i+49])\n",
    "\n",
    "    arrayofdata50=np.array(dataof50)\n",
    "    print(arrayofdata50.shape)\n",
    "    sum1=[]\n",
    "    for channel in arrayofdata50:\n",
    "        sum1.append(np.sum(channel))\n",
    "    sum1=np.array(sum1)\n",
    "    print(\"shape of channel sum : Xi\", sum1.shape , sum1[0:3])\n",
    "    mean1=np.sum(sum1)/125\n",
    "    print(\"mean of 125 channel: X\",mean1)\n",
    "    Dev=sum1-mean1\n",
    "    print(\"Xi-X\",Dev.shape,Dev[0:3])\n",
    "    SquareofDev=Dev*Dev\n",
    "    print(\"(Xi-X)^2\",SquareofDev.shape,SquareofDev[0:3])\n",
    "    ##print(\"variance\",var1)\n",
    "    sumofdev=np.sum(SquareofDev)\n",
    "    print(\"sum of (Xi-X)^2\",sumofdev)\n",
    "    var=sumofdev/125\n",
    "    print(\"variance\",var)\n",
    "     \n",
    "    index = np.arange(125)\n",
    "    plt.bar(index, SquareofDev)\n",
    "    plt.xlabel('Genre', fontsize=5)\n",
    "    plt.ylabel('No of Movies', fontsize=5)\n",
    "    plt.xticks(index, SquareofDev, fontsize=5, rotation=30)\n",
    "    plt.title('Square of data from average')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n",
    "newdata[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,1))\n",
    "print(getave(newdata,51,1))\n",
    "print(getave(newdata,101,1))\n",
    "print(getave(newdata,151,1))\n",
    "print(getave(newdata,201,1))\n",
    "print(getave(newdata,251,1))\n",
    "print(getave(newdata,301,1))\n",
    "print(getave(newdata,351,1))\n",
    "print(getave(newdata,401,1))\n",
    "print(getave(newdata,451,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,50,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,0))\n",
    "print(getave(newdata,51,0))\n",
    "print(getave(newdata,101,0))\n",
    "print(getave(newdata,151,0))\n",
    "print(getave(newdata,201,0))\n",
    "print(getave(newdata,251,0))\n",
    "print(getave(newdata,301,0))\n",
    "print(getave(newdata,351,0))\n",
    "print(getave(newdata,401,0))\n",
    "print(getave(newdata,451,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,3))\n",
    "print(getave(newdata,51,3))\n",
    "print(getave(newdata,101,3))\n",
    "print(getave(newdata,151,3))\n",
    "print(getave(newdata,201,3))\n",
    "print(getave(newdata,251,3))\n",
    "print(getave(newdata,301,3))\n",
    "print(getave(newdata,351,3))\n",
    "print(getave(newdata,401,3))\n",
    "print(getave(newdata,451,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,4))\n",
    "print(getave(newdata,51,4))\n",
    "print(getave(newdata,101,4))\n",
    "print(getave(newdata,151,4))\n",
    "print(getave(newdata,201,4))\n",
    "print(getave(newdata,251,4))\n",
    "print(getave(newdata,301,4))\n",
    "print(getave(newdata,351,4))\n",
    "print(getave(newdata,401,4))\n",
    "print(getave(newdata,451,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,1))\n",
    "print(getave(newdata,51,1))\n",
    "print(getave(newdata,101,1))\n",
    "print(getave(newdata,151,1))\n",
    "print(getave(newdata,201,1))\n",
    "print(getave(newdata,251,1))\n",
    "print(getave(newdata,301,1))\n",
    "print(getave(newdata,351,1))\n",
    "print(getave(newdata,401,1))\n",
    "print(getave(newdata,451,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch[0:10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,0,35))\n",
    "print(getave(newdata,51,35))\n",
    "print(getave(newdata,101,35))\n",
    "print(getave(newdata,151,35))\n",
    "print(getave(newdata,201,35))\n",
    "print(getave(newdata,251,35))\n",
    "print(getave(newdata,301,35))\n",
    "print(getave(newdata,351,35))\n",
    "print(getave(newdata,401,35))\n",
    "print(getave(newdata,451,35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(getave(newdata,31,2))\n",
    "print(getave(newdata,31,923))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataof50=[]\n",
    "for channel in newdata[1]:\n",
    "    dataof50.append(channel[0:100])\n",
    "\n",
    "arrayofdata50=np.array(dataof50)\n",
    "print(arrayofdata50.shape)\n",
    "sum1=[]\n",
    "sum2=[]\n",
    "for channel in arrayofdata50:\n",
    "    sum1.append(np.sum(channel))\n",
    "sum1=np.array(sum1)\n",
    "print(\"shape of channel sum : Xi\", sum1.shape , sum1[0:3])\n",
    "mean1=np.sum(sum1)/125\n",
    "print(\"mean of 125 channel: X\",mean1)\n",
    "Dev=sum1-mean1\n",
    "print(\"Xi-X\",Dev.shape,Dev[0:3])\n",
    "SquareofDev=Dev*Dev\n",
    "print(\"(Xi-X)^2\",SquareofDev.shape,SquareofDev[0:3])\n",
    "    ##print(\"variance\",var1)\n",
    "sumofdev=np.sum(SquareofDev)\n",
    "print(\"sum of (Xi-X)^2\",sumofdev)\n",
    "var=sumofdev/125\n",
    "print(\"variance\",var)\n",
    "std=np.sqrt(var)\n",
    "print(\"std\",std)\n",
    "dis1=mean1-std\n",
    "dis2=mean1+std\n",
    "print(dis1,dis2)\n",
    "i=0\n",
    "index=[]\n",
    "for i in range(len(sum1)):\n",
    "    if sum1[i]<dis1 or sum1[i]>dis2 :\n",
    "        sum2.append(sum1[i])\n",
    "        index.append([i])\n",
    "sum2=np.array(sum2)\n",
    "print(sum2.shape)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findchanel (newdata, i,j):\n",
    "    dataof50=[]\n",
    "    for channel in newdata[j]:\n",
    "        dataof50.append(channel[i:i+499])\n",
    "\n",
    "    arrayofdata50=np.array(dataof50)\n",
    "    print(arrayofdata50.shape)\n",
    "    sum1=[]\n",
    "    sum2=[]\n",
    "    for channel in arrayofdata50:\n",
    "        sum1.append(np.sum(channel))\n",
    "    sum1=np.array(sum1)\n",
    "    print(\"shape of channel sum : Xi\", sum1.shape , sum1[0:3])\n",
    "    mean1=np.sum(sum1)/125\n",
    "    print(\"mean of 125 channel: X\",mean1)\n",
    "    Dev=sum1-mean1\n",
    "    print(\"Xi-X\",Dev.shape,Dev[0:3])\n",
    "    SquareofDev=Dev*Dev\n",
    "    print(\"(Xi-X)^2\",SquareofDev.shape,SquareofDev[0:3])\n",
    "        ##print(\"variance\",var1)\n",
    "    sumofdev=np.sum(SquareofDev)\n",
    "    print(\"sum of (Xi-X)^2\",sumofdev)\n",
    "    var=sumofdev/125\n",
    "    print(\"variance\",var)\n",
    "    std=np.sqrt(var)\n",
    "    print(\"std\",std)\n",
    "    dis1=mean1-std\n",
    "    dis2=mean1+std\n",
    "    print(dis1,dis2)\n",
    "    k=0\n",
    "    index=[]\n",
    "    for k in range(len(sum1)):\n",
    "        if sum1[k]<dis1 or sum1[k]>dis2 :\n",
    "            sum2.append(sum1[k])\n",
    "            index.append([k])\n",
    "    sum2=np.array(sum2)\n",
    "    print(sum2.shape)\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findchanel (newdata, i,j):\n",
    "    dataof50=[]\n",
    "    for channel in newdata[j]:\n",
    "        dataof50.append(channel[i:i+49])\n",
    "\n",
    "    arrayofdata50=np.array(dataof50)\n",
    "    print(arrayofdata50.shape)\n",
    "    sum1=[]\n",
    "    sum2=[]\n",
    "    for channel in arrayofdata50:\n",
    "        sum1.append(np.sum(channel))\n",
    "    sum1=np.array(sum1)\n",
    "    #print(\"shape of channel sum : Xi\", sum1.shape , sum1[0:3])\n",
    "    mean1=np.sum(sum1)/125\n",
    "    #print(\"mean of 125 channel: X\",mean1)\n",
    "    Dev=sum1-mean1\n",
    "    #print(\"Xi-X\",Dev.shape,Dev[0:3])\n",
    "    SquareofDev=Dev*Dev\n",
    "    #print(\"(Xi-X)^2\",SquareofDev.shape,SquareofDev[0:3])\n",
    "        ##print(\"variance\",var1)\n",
    "    sumofdev=np.sum(SquareofDev)\n",
    "    #print(\"sum of (Xi-X)^2\",sumofdev)\n",
    "    var=sumofdev/125\n",
    "    #print(\"variance\",var)\n",
    "    std=np.sqrt(var)\n",
    "    #print(\"std\",std)\n",
    "    dis1=mean1-std\n",
    "    dis2=mean1+std\n",
    "    print(dis1,dis2)\n",
    "    k=0\n",
    "    index=[]\n",
    "    for k in range(len(sum1)):\n",
    "        if sum1[k]<dis1 or sum1[k]>dis2 :\n",
    "            sum2.append(sum1[k])\n",
    "            index.append([k])\n",
    "    sum2=np.array(sum2)\n",
    "    print(sum2.shape)\n",
    "    print(index)\n",
    "    stdup=[dis1]*125\n",
    "    stddown=[dis2]*125\n",
    "#     x=np.arange(0,125,1)\n",
    "#     _=plt.plot(x,sum1)\n",
    "#     _=plt.plot(x,stdup)\n",
    "#     _=plt.plot(x,stddown)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,1)\n",
    "findchanel (newdata, 50,1)\n",
    "findchanel (newdata, 100,1)\n",
    "findchanel (newdata, 150,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,2)\n",
    "findchanel (newdata, 50,2)\n",
    "findchanel (newdata, 100,2)\n",
    "findchanel (newdata, 150,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "findchanel (newdata, 0,0)\n",
    "findchanel (newdata, 0,1)\n",
    "findchanel (newdata, 0,2)\n",
    "findchanel (newdata, 0,3)\n",
    "findchanel (newdata, 0,4)\n",
    "findchanel (newdata, 0,5)\n",
    "findchanel (newdata, 0,6)\n",
    "findchanel (newdata, 0,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feathres selection, crrelation  ,shot gun approach, \n",
    "add 1000 random column, feathres selection, crrelation  ,shot gun approach, add 1000 random column, 'find most collration -colllerate '\n",
    "random forest.\n",
    "random forest take features:\n",
    "    take the data, find the time and location of channel which are most collerate\n",
    "    redecued the number of fueatures.\n",
    "    look for different feature selection methods\n",
    "    certain points to your label\n",
    "    colleation average over all the trainning data\n",
    "    stronger the outline\n",
    "    botton line: \n",
    "    not neccessary find classfier for visual vs studio\n",
    "    when there's congruent and non congruent, this is difference about the brain, when and where.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
