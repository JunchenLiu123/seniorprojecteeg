{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce the data points for each channel\n",
    "* this notebook will take the average of each 50 points and convert it to 1 point, so 500 ms will now become 10 ms\n",
    "* The goal is to be able to simplify the data from nevent,nchanel,500 to nevent,nchanel,10 and do classfication\n",
    "  * in other words it will be easier for us to find the variances acorss channels by having less data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mne.decoding import SPoC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n",
    "                          cross_val_multiscore, LinearModel, get_coef)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE \n",
    "import os\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "import os.path as op\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from scipy.stats import skew, kurtosis\n",
    "import mne \n",
    "\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC  # noqa\n",
    "from sklearn.model_selection import ShuffleSplit  # noqa\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mne.viz import tight_layout\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Batch_ArtifactFilter_Epoch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=remove_artifacts_and_create_epochs('D:\\\\USB Drive\\\\NewEEG-200s\\\\SA20140205\\\\20140205_1230.set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data()[1,1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data()[1,1,10:20].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape # get shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()[1,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=50\n",
    "j=0\n",
    "newdata1=[]\n",
    "# loop through the data and change the last dimension of the data from 500 to 10.\n",
    "for i in range(len(newdata)):\n",
    "    if i%50==0:\n",
    "        newdata1[1,1,j]=np.mean(newdata[1,1,i-50:i])\n",
    "        j=j+1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=epoch.get_data()[1,1,0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=epoch.get_data()[1,1,51:100]\n",
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " final_variances = []\n",
    "### methods for converting the 500 points to 3 points : kurtosis, skewness,variance\n",
    "    for d in data:\n",
    "    #     print (data.shape)\n",
    "        variances = []\n",
    "        skewnesses = []\n",
    "        kurtosises = []\n",
    "        stats = []\n",
    "        for channel in \n",
    "    #         print(channel.shape)\n",
    "            var = channel.var()\n",
    "            variances.append(var)\n",
    "            \n",
    "            skewness = skew(channel, axis=0)\n",
    "#             print(skewness)\n",
    "\n",
    "            skewnesses.append(skewness)\n",
    "#             print(skewness)\n",
    "            kurt = kurtosis(channel)\n",
    "            kurtosises.append(kurt)\n",
    "#         print(skewnesses)\n",
    "#         mean_skew = np.mean(skewnesses)\n",
    "#         stats.append(mean_skew)\n",
    "#         stats.append(np.mean(variances))\n",
    "        final_variances.append([np.mean(variances), np.mean(skewnesses), np.mean(kurtosises)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print the shape the channel\n",
    "for d in newdata:\n",
    "    for channel in d:\n",
    "        print(channel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_variances = []\n",
    "final_variances.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n",
    "data=[]\n",
    "## methods for converting 500 points to 10 points by taking the average of each 50 points\n",
    "for d in newdata:\n",
    "    mean1=[]\n",
    "    mean2=[]\n",
    "    mean3=[]\n",
    "    mean4=[]\n",
    "    mean5=[]\n",
    "    mean6=[]\n",
    "    mean7=[]\n",
    "    mean8=[]\n",
    "    mean9=[]\n",
    "    mean10=[]\n",
    "    #   print (data.shape)\n",
    "    for channel in d:\n",
    "        mean1.append(np.mean(channel[0:50]))\n",
    "        mean2.append(np.mean(channel[51:100]))\n",
    "        mean3.append(np.mean(channel[101:150]))\n",
    "        mean4.append(np.mean(channel[151:200]))\n",
    "        mean5.append(np.mean(channel[201:250]))\n",
    "        mean6.append(np.mean(channel[251:300]))\n",
    "        mean7.append(np.mean(channel[301:350]))\n",
    "        mean8.append(np.mean(channel[351:400]))\n",
    "        mean9.append(np.mean(channel[401:450]))\n",
    "        mean10.append(np.mean(channel[451:500]))\n",
    "    data.append([mean1,mean2,mean3,mean4,mean5,mean6,mean7,mean8,mean9,mean10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array= np.array(final_variances)\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch[0:10].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array(data)\n",
    "olddata=newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=data.swapaxes(1,2)## swapping the axes of second and third dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n",
    "data=[]\n",
    "for d in newdata:\n",
    "    mean1=[]\n",
    "    mean2=[]\n",
    "    mean3=[]\n",
    "    mean4=[]\n",
    "    mean5=[]\n",
    "    mean6=[]\n",
    "    mean7=[]\n",
    "    mean8=[]\n",
    "    mean9=[]\n",
    "    mean10=[]\n",
    "    #   print (data.shape)\n",
    "    for channel in d:\n",
    "        mean1.append(channel[0:50])\n",
    "        mean2.append(channel[51:100])\n",
    "        mean3.append(channel[101:150])\n",
    "        mean4.append(channel[151:200])\n",
    "        mean5.append(channel[201:250])\n",
    "        mean6.append(channel[251:300])\n",
    "        mean7.append(channel[301:350])\n",
    "        mean8.append(channel[351:400])\n",
    "        mean9.append(channel[401:450])\n",
    "        mean10.append(channel[451:500])\n",
    "    data.append([mean1,mean2,mean3,mean4,mean5,mean6,mean7,mean8,mean9,mean10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2eaa2a443281>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Genre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No of Movies'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## Print the means of the average between each channel outs\n",
    "index = np.arange(len(newdata[1,1,:]))\n",
    "plt.bar(index, newdata[1,1,:])\n",
    "plt.xlabel('Genre', fontsize=5)\n",
    "plt.ylabel('No of Movies', fontsize=5)\n",
    "plt.xticks(index, newdata[1,1,:], fontsize=5, rotation=30)\n",
    "plt.title('average 10 points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "#store 50 data points in an array\n",
    "for epoch in data:\n",
    "    for channel in epoch:\n",
    "        for i in range(len(channel)):\n",
    "            if i%50 == 0:\n",
    "                section[j]==channel[i-50:i]\n",
    "                j++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()\n",
    "dataof50=[]\n",
    "\n",
    "for channel in newdata[0]:\n",
    "    dataof50.append(channel[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#methods for finding the vriance for each 50 data points\n",
    "def getave(data,i,j):\n",
    "    dataof100=[]\n",
    "    for channel in data[j]:\n",
    "        dataof100.append(channel[i:i+49])\n",
    "\n",
    "    arrayofdata100=np.array(dataof100)\n",
    "    arrayofdata100.shape\n",
    "    sum1=[]\n",
    "    for channel in arrayofdata100:\n",
    "        sum1.append(np.sum(channel))\n",
    "    print(np.array(sum1).shape)\n",
    "    sum1=np.array(sum1)\n",
    "    mean1=np.mean(sum1)\n",
    "    print(mean1)\n",
    "    var1=sum1-mean1\n",
    "    print(var1.shape)\n",
    "    average1=np.mean(var1)\n",
    "    print(average1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,101)# get the variance between data points 101 to 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for finding the standard deviation for each 50 points\n",
    "def getave(data,i,j):\n",
    "    dataof100=[]\n",
    "    for channel in data[j]:\n",
    "        dataof100.append(channel[i:i+49])\n",
    "\n",
    "    arrayofdata100=np.array(dataof100)\n",
    "    print(\"shape of array\",arrayofdata100.shape)\n",
    "    sum1=[]\n",
    "    for channel in arrayofdata100:\n",
    "        sum1.append(np.sum(channel))\n",
    "    print(\"shape of array sum\",np.array(sum1).shape)\n",
    "    sum1=np.array(sum1)\n",
    "    print(\"sum\", np.sum(sum1))\n",
    "    mean1=np.mean(sum1)\n",
    "    mean2=np.sum(sum1)/129\n",
    "    var1=sum1-mean1\n",
    "    print(var1)\n",
    "    average1=np.mean(var1)\n",
    "    print(average1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,101,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,101,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.drop_channels(['STI 014'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.drop_channels(['Lm','Rm','VEOG'])## Drop bad channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=epoch.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for finding the variance of each 50 data points\n",
    "def getave(data,i,j):\n",
    "    dataof100=[]\n",
    "    for channel in data[j]:\n",
    "        dataof100.append(channel[i:i+50])\n",
    "\n",
    "    arrayofdata100=np.array(dataof100)\n",
    "    print(arrayofdata100.shape)\n",
    "    sum1=[]\n",
    "    for channel in arrayofdata100:\n",
    "        sum1.append(np.sum(channel))\n",
    "    sum1=np.array(sum1)\n",
    "    mean1=np.mean(sum1)\n",
    "    mean2=np.sum(sum1)/129\n",
    "    var1=sum1-mean1\n",
    "    print(\"variance\",var1)\n",
    "    average1=np.mean(var1)\n",
    "    print(\"average varaince for segment \" ,i ,\"to \", i+49 , \"is: \" ,average1)\n",
    "#     df=pd.DataFrame(var1)\n",
    "#     print(df)\n",
    "#     df.columns=['variance']\n",
    "#     index = np.arange(125)\n",
    "#     df.plot.bar(x=index,y='variance')\n",
    "    \n",
    "    index = np.arange(125)\n",
    "    plt.bar(index, var1)\n",
    "    plt.xlabel('Genre', fontsize=5)\n",
    "    plt.ylabel('No of Movies', fontsize=5)\n",
    "    plt.xticks(index, var1, fontsize=5, rotation=30)\n",
    "    plt.title('average 10 points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,151,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,0,1)\n",
    "getave(newdata,51,1)\n",
    "getave(newdata,101,1)\n",
    "getave(newdata,151,1)\n",
    "getave(newdata,201,1)\n",
    "getave(newdata,251,1)\n",
    "getave(newdata,301,1)\n",
    "getave(newdata,351,1)\n",
    "getave(newdata,401,1)\n",
    "getave(newdata,451,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getave(newdata,0,4)\n",
    "getave(newdata,51,4)\n",
    "getave(newdata,101,4)\n",
    "getave(newdata,151,4)\n",
    "getave(newdata,201,4)\n",
    "getave(newdata,251,4)\n",
    "getave(newdata,301,4)\n",
    "getave(newdata,351,4)\n",
    "getave(newdata,401,4)\n",
    "getave(newdata,451,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the variance of each channels\n",
    "getave(newdata,51,0)\n",
    "index = np.arange(129)\n",
    "plt.bar(index, var1)\n",
    "plt.xlabel('Genre', fontsize=5)\n",
    "plt.ylabel('No of Movies', fontsize=5)\n",
    "plt.xticks(index, var1, fontsize=5, rotation=30)\n",
    "plt.title('average 10 points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
