{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final presentation\n",
    "* visualization of the brain area\n",
    "\n",
    "## note: Run on CCNY computing node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from mne.decoding import SPoC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import mne\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from mne.datasets import sample\n",
    "from mne.decoding import (SlidingEstimator, GeneralizingEstimator,\n",
    "                          cross_val_multiscore, LinearModel, get_coef)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.manifold import TSNE \n",
    "import os\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "import os.path as op\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import defaultdict\n",
    "from scipy.stats import skew, kurtosis\n",
    "import mne \n",
    "\n",
    "from mne.decoding import Vectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC  # noqa\n",
    "from sklearn.model_selection import ShuffleSplit  # noqa\n",
    "\n",
    "from mne.decoding import UnsupervisedSpatialFilter\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mne.viz import tight_layout\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from urllib.request import urlopen\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../LARGE_EPOCH_OBJECTS/large_epoch_1_epo-1.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8642da55a7e1>:1: RuntimeWarning: This filename (../LARGE_EPOCH_OBJECTS/large_epoch_1_epo-1.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epoch1=mne.read_epochs('../LARGE_EPOCH_OBJECTS/large_epoch_1_epo-1.fif')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8103 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "8103 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Reading ../LARGE_EPOCH_OBJECTS/large_epoch_1_epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "8103 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Reading ../LARGE_EPOCH_OBJECTS/large_epoch_1_epo-1.fif ...\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...     500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "8103 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "16206 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "epoch1=mne.read_epochs('../LARGE_EPOCH_OBJECTS/large_epoch_1_epo-1.fif')\n",
    "epoch2=mne.read_epochs('../LARGE_EPOCH_OBJECTS/large_epoch_1_epo.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24309 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "# Concatenate Epoch\n",
    "epoch=mne.concatenate_epochs([epoch1,epoch2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Nasium' in epoch.ch_names:\n",
    "    epoch.drop_channels(ch_names=['Nasium', 'LL4', 'L12', 'VEOG','STI 014']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24309, 124, 257)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch.get_data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# epoch object has been updated with events that can be used for classification\n",
    "* epoch.events[:,-1] in {112, 312, 512, 712, 912, 1112} = Language\n",
    "* epoch.events[:,-1] in {212, 412, 612, 812, 1012, 1212} = Non Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = [112, 312, 512, 712, 912, 1112]\n",
    "non_language = [212, 412, 612, 812, 1012, 1212]\n",
    "\n",
    "labels = [0 if (x in language) else 1 for x in epoch.events[:,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24309,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_np = np.array(labels)\n",
    "labels_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24309,)\n",
      "(24309, 124, 257) (24309, 124, 257)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(24309, 31868)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = epoch.get_data()\n",
    "y = labels_np\n",
    "print(labels_np.shape)\n",
    "print(X.shape,epoch.get_data().shape)\n",
    "vec=Vectorizer()\n",
    "X=vec.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators =110, random_state = 42)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 42)\n",
    "    \n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked = mne.EvokedArray(channel, epoch.info, tmin=epoch.times[0])\n",
    "joint_kwargs = dict(ts_args=dict(time_unit='s'),\n",
    "                    topomap_args=dict(time_unit='s'))\n",
    "evoked.plot_joint(times=[0.030,0.13,0.205,0.300], title='patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the shape of feature important back to 124*257 to get the feature importance of each time points\n",
    "channel=[]\n",
    "importances = np.array(clf.feature_importances_)\n",
    "for i in range(0,len(importances)):\n",
    "    if i%257 != 0:\n",
    "        n_sample.append(importances[i])\n",
    "    else:\n",
    "        n_sample=[]\n",
    "        n_sample.append(importances[i])\n",
    "        channel.append(n_sample)\n",
    "channel=np.array(channel)\n",
    "print(channel.shape,importances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the features impoartance using evoke\n",
    "evoked = mne.EvokedArray(channel, epoch.info, tmin=epoch.times[0])\n",
    "joint_kwargs = dict(ts_args=dict(time_unit='s'),\n",
    "                    topomap_args=dict(time_unit='s'))\n",
    "evoked.plot_joint(times=[0.030,0.13,0.205,0.300], title='patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ranking Channels by feature importances\n",
    "channel=[]\n",
    "importances = np.array(clf.feature_importances_)\n",
    "for i in range(0,len(importances),257):\n",
    "    channel.append(np.sum(importances[i:i+257]))\n",
    "    channel1=np.array(channel)\n",
    "    channel1.shape\n",
    "import pandas as pd\n",
    "feature_imp = pd.Series(channel1,index=epoch.ch_names).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the top 20 most important channels on head\n",
    "name=feature_imp.index[0:20]\n",
    "np.array(name)\n",
    "epoch.plot_sensors(show_names=name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
